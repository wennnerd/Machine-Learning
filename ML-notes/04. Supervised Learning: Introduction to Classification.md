# Supervised Learning: Introduction to Classification

**Contents**

[K-Nearest Neighbors](#knn)

[Accuracy, Recall, Precision and F1 score](#effectiveness)

[Logistic Regression](#lr)


-----

# K-Nearest Neighbors  <a name="knn"/>

**Contents**

[K-Nearest Neighbor Algorithm](#knn-alg)

-----

## K-Nearest Neighbor Algorithm <a name="knn-alg"/>

K-Nearest Neighbors (KNN) is a classification algorithm. The central idea is that data points with similar attributes tend to fall into similar categories.  

K-Nearest Neighbor Algorithm:

1. **Normalized the data**:  
    * min-max normalized:
    
      <img src="http://chart.googleapis.com/chart?cht=tx&chl= \frac{value-min}{max-min}" >

    * z-score normalized:
    
      <img src="http://chart.googleapis.com/chart?cht=tx&chl= \frac{value-\mu}{\sigma}" >
2. **Find the _k_ nearest neighbors**:

      Now that our data has been normalized and we know how to find the distance between two points, we can begin classifying unknown data!
      
3. **Classify the new point based on those neighbors**:
      



-----

# Accuracy, Recall, Precision and F1 score <a name="effectiveness"/>

**Contents**

[Accuracy](#acc)

[Recall](#recall)

[Precision](#pre)

[F1 Score](#f1)

[Scikit-Learn](#sklearn-eff)

-----
Consider that using machine learning algorithm to predict whether or not you will get above a B on a test. Then we have the following feature:  
* The number of hours you studied this week.  
* The number of hours you watched Netflix this week.  
* The time you went to bed the night before the test.  
* Your average in the class before taking the test.  

First, define our terms of grade:  
* True Positive: The algorithm predicted you would get above a B, and you did.
* True Negative: The algorithm predicted you would get below a B, and you did.
* False Positive: The algorithm predicted you would get above a B, and you didn’t.
* False Negative: The algorithm predicted you would get below a B, and you didn’t.

<table>
    <tr>
        <td > </td> 
        <center><td colspan="3">Predict Class</td></center>
   </tr>
    <tr>
        <td rowspan="3">Actual Class</td>
        <td > </td> 
        <td >Positive</td> 
        <td >Negative</td> 
    </tr>
    <tr>
        <td >Positive</td> 
        <td >True Positive</td> 
        <td >False Negative</td> 
    </tr>
    <tr>
        <td >Negative</td>
        <td >False Positive</td>  
        <td >True Negative</td>
    </tr>
</table>

## Accuracy <a name="acc"/>

在所有情況中，正確判斷真假的比例

<img src="http://chart.googleapis.com/chart?cht=tx&chl= accuracy = \frac{TP %2B TN}{TP %2B FN %2B FP %2B TN} + " >

## Recall <a name="recall"/>

為真的情況下，有多少被正確判斷出來

<img src="http://chart.googleapis.com/chart?cht=tx&chl= recall = \frac{TP}{TP %2B FN} + " >

## Precision <a name="pre"/>

判斷為真的情況下，有多少是真的真

<img src="http://chart.googleapis.com/chart?cht=tx&chl= precision = \frac{TP}{TP %2B FP} + " >

## F1 score <a name="f1"/>

如果今天我覺得Precision和Recall都同等重要，我想要用一個指標來統合標誌它，這就是F1 Score

<img src="http://chart.googleapis.com/chart?cht=tx&chl= F1 = 2*\frac{Precision*Recall}{Precision%2BRecall}" >

## Scikit-Learn <a name="sklearn-eff"/>

```py
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score
```

```py
labels = [1, 0, 0, 1, 1, 1, 0, 1, 1, 1]
guesses = [0, 1, 1, 1, 1, 0, 1, 0, 1, 0]

print(accuracy_score(labels, guesses)) # Accuracy
print(recall_score(labels, guesses)) # Recall
print(precision_score(labels, guesses)) # Precision
print(f1_score(labels, guesses)) # F1 score
```

-----

# Logistic Regression <a name="lr"/>

**Contents**

[Log-Odds](#logodds)   

[Sigmoid Function](#sigf)  

[Log Loss](#logloss)  

[Classification Thresholding](#classthre)  

[Scikit-Learn](#sklearn-lr)

[Feature Importance](#fi)

-----

Consider an simple data which ```y``` is either 1 (passing), or 0 (failing), and we have one feature, ```num_hours_studied```.
In this example, we want to predict the propability of passing which does not provided by output of **Linear Regression model**. Therefore, we step in **Logistic Regression**.

To predict the probability of a data sample belonging to a class:
1. initialize all feature coefficients and intercept to 0.  
2. multiply each of the feature coefficients by their respective feature value to get what is known as the **log-odds**.  
3. place the log-odds into the **sigmoid function** to link the output to the range [0,1], giving us a probability.  

## Log-Odds <a name="logodds"/>

Calculate the odds of an event occurring as follows:

<img src="http://chart.googleapis.com/chart?cht=tx&chl= Odds = \frac{P(success)}{P(fail)}" >, <img src="http://chart.googleapis.com/chart?cht=tx&chl= log-odds = log( \frac{P(success)}{P(fail)} )" >

Logistic regression model:

<img src="http://chart.googleapis.com/chart?cht=tx&chl= log(\frac{p}{1-p}) = \beta_{0}%2B\beta_{1}x_{1}%2B\cdots%2B\beta_{k}x_{k}" >

Therefore, 

<img src="http://chart.googleapis.com/chart?cht=tx&chl= p = \frac{exp(\beta_{0}%2B\beta_{1}x_{1}%2B\cdots%2B\beta_{k}x_{k})}{1 %2B exp(\beta_{0}%2B\beta_{1}x_{1}%2B\cdots%2B\beta_{k}x_{k})}" > 

which <img src="http://chart.googleapis.com/chart?cht=tx&chl= p" > is the probability of success.

**Python Code:**  
```py
def log_odds(features, coefficients,intercept):
  return np.dot(features,coefficients) + intercept
```

## Sigmoid function <a name="sigf"/>

The Sigmoid Function is a special case of the more general Logistic Function, where Logistic Regression gets its name.

<img src="http://chart.googleapis.com/chart?cht=tx&chl= h(z) = \frac{1}{1 %2B exp(-z)}" >

By plugging the log-odds into the Sigmoid Function, defined below, we map the log-odds to the range [0,1].

**Python Code:**  
```py
def sigmoid(z):
    denominator = 1 + np.exp(-z)
    return 1/denominator
```

## Log Loss <a name="logloss"/>

The loss function for Logistic Regression, known as Log Loss, is given below:

<img src="http://chart.googleapis.com/chart?cht=tx&chl= -\frac{1}{m}\sum_{i=1}^{m}[y^{(i)}log(h(z^{(i)})) %2B (1-y^{(i)})log(1-h(z^{(i)}))]" >

**Python Code:**  
```py
def log_loss(probabilities,actual_class):
  return np.sum(-(1/actual_class.shape[0])*(actual_class*np.log(probabilities) + (1-actual_class)*np.log(1-probabilities)))
 ```
 
## Classification Thresholding <a name="classthre"/>

Many machine learning algorithms, including Logistic Regression, spit out a classification probability as their result. Once we have this probability, we need to make a decision on what class the sample belongs to. This is where the classification threshold comes in. We can choose to change the threshold of classification based on the use-case of our model.  

## Scikit-Learn <a name="sklearn-lr"/>

```py
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(features, labels)
```
Use ```.fit()``` to train our model, we can access a few useful attributes of the LogisticRegression object.  
* ```model.coef_``` is a vector of the coefficients of each feature  
* ```model.intercept_``` is the intercept   

With our trained model we are able to predict whether new data points belong to the positive class using the ```.predict()``` method! ```.predict()``` takes a matrix of features as a parameter and returns a vector of labels 1 or 0 for each sample.

```py
model.predict(features)
```

If we are more interested in the predicted probability of the data samples belonging to the positive class than the actual class, we can use the ```.predict_proba()``` method. 

```py
model.predict_proba(features)
```

## Feature Importance <a name="fi"/>

Discuss how to interpret the feature coefficients from a logistic model. We gives a normalized data to help us understand the meaning of feature cofficients:

* Features with larger, positive coefficients will increase the probability of a data sample belonging to the positive class  
* Features with larger, negative coefficients will decrease the probability of a data sample belonging to the positive class  
* Features with small, positive or negative coefficients have minimal impact on the probability of a data sample belonging to the positive class  
