# Supervised Learning: Regression

**Contents**

[Simple Linear Regression](#SLR)

[Multiple Linear Regression](#MLR)

## Simple Linear Regression <a name="SLR"/>

[Distance Formula](#DF)

[Linear Regression](#LR)

### Distance Formula <a name="DF"/>

1. **Euclidean Distance:**

```python
def euclidean_distance(pt1, pt2):
  distance = 0
  for i in range(len(pt1)):
    distance += (pt1[i] - pt2[i]) ** 2
  return distance ** 0.5
  
print(euclidean_distance([1, 2], [4, 0]))
```

2. **Manhattan Distance:**

```python
def manhattan_distance(pt1, pt2):
  distance = 0
  for i in range(len(pt1)):
    distance += abs(pt1[i] - pt2[i])
  return distance
  
print(manhattan_distance([1, 2], [4, 0]))
```

3. **Hamming Distance:**

```python
def hamming_distance(pt1, pt2):
  distance = 0
  for i in range(len(pt1)):
    if pt1[i] != pt2[i]:
      distance += 1
  return distance
  
print(hamming_distance([5, 4, 9], [1, 7, 9]))
```
#### Scikit-Learn

Now that you’ve written these three distance formulas yourself, let’s look at how to use them using Python’s SciPy library:

Euclidean Distance: ```.euclidean() ```

Manhattan Distance: ```.cityblock() ```

Hamming Distance: ```.hamming() ```

```python
from scipy.spatial import distance
print(distance.euclidean([1,2],[4,0]))
print(distance.cityblock([1,2],[4,0]))
print(distance.hamming([5, 4, 9], [1, 7, 9]))
```

### Linear Regression <a name="LR"/>

**Gradient Descent for Intercept**

```python
def get_gradient_at_b(x, y, m, b):
    diff = 0
    N = len(x)
    for i in range(N):
      y_val = y[i]
      x_val = x[i]
      diff += (y_val - ((m * x_val) + b))
    b_gradient = -2/N * diff
    return b_gradient
```

**Gradient Descent for Slope**

```python
def get_gradient_at_m(x, y, m, b):
  diff = 0
  N = len(x)
  for i in range(N):
    y_val = y[i]
    x_val = x[i]
    diff += x_val*(y_val - ((m * x_val) + b))
  m_gradient = -2/N * diff
  return m_gradient
```

**Step Gradient Descent**

```python
def step_gradient(b_current, m_current, x, y, learning_rate):
    b_gradient = get_gradient_at_b(x, y, b_current, m_current)
    m_gradient = get_gradient_at_m(x, y, b_current, m_current)
    b = b_current - (learning_rate * b_gradient)
    m = m_current - (learning_rate * m_gradient)
    return [b, m]
```


## Multiple Linear Regression <a name="MLR"/>


