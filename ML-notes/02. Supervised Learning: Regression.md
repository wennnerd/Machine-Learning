# Supervised Learning: Regression

**Contents**

[Simple Linear Regression](#SLR)

[Multiple Linear Regression](#MLR)

## Simple Linear Regression <a name="SLR"/>

[Distance Formula](#DF)

[Gradient Descent](#GD)

### Distance Formula <a name="DF"/>

1. **Euclidean Distance:**

```python
def euclidean_distance(pt1, pt2):
  distance = 0
  for i in range(len(pt1)):
    distance += (pt1[i] - pt2[i]) ** 2
  return distance ** 0.5
  
print(euclidean_distance([1, 2], [4, 0]))
```

2. **Manhattan Distance:**

```python
def manhattan_distance(pt1, pt2):
  distance = 0
  for i in range(len(pt1)):
    distance += abs(pt1[i] - pt2[i])
  return distance
  
print(manhattan_distance([1, 2], [4, 0]))
```

3. **Hamming Distance:**

```python
def hamming_distance(pt1, pt2):
  distance = 0
  for i in range(len(pt1)):
    if pt1[i] != pt2[i]:
      distance += 1
  return distance
  
print(hamming_distance([5, 4, 9], [1, 7, 9]))
```
**Scikit-Learn**

Now that you’ve written these three distance formulas yourself, let’s look at how to use them using Python’s SciPy library:

Euclidean Distance: ```.euclidean() ```

Manhattan Distance: ```.cityblock() ```

Hamming Distance: ```.hamming() ```

```python
from scipy.spatial import distance
print(distance.euclidean([1,2],[4,0]))
print(distance.cityblock([1,2],[4,0]))
print(distance.hamming([5, 4, 9], [1, 7, 9]))
```

### Gradient Descent <a name="GD"/>

**Gradient Descent for Intercept**

```python
def get_gradient_at_b(x, y, m, b):
  diff = 0
  N = len(x)
  for i in range(N):
    y_val = y[i]
    x_val = x[i]
    diff += (y_val - ((m * x_val) + b))
  b_gradient = -2/N * diff
  return b_gradient
```

**Gradient Descent for Slope**

```python
def get_gradient_at_m(x, y, m, b):
  diff = 0
  N = len(x)
  for i in range(N):
    y_val = y[i]
    x_val = x[i]
    diff += x_val*(y_val - ((m * x_val) + b))
  m_gradient = -2/N * diff
  return m_gradient
```

**Step Gradient Descent**

```python
def step_gradient(b_current, m_current, x, y, learning_rate):
  b_gradient = get_gradient_at_b(x, y, b_current, m_current)
  m_gradient = get_gradient_at_m(x, y, b_current, m_current)
  b = b_current - (learning_rate * b_gradient)
  m = m_current - (learning_rate * m_gradient)
  return [b, m]
```

**Gradient Descent Function**

```python
def gradient_descent(x, y, learning_rate, num_iterations):
  b, m = 0, 0
  for i in range(num_iterations):
    b, m = step_gradient(b, m, x, y, learning_rate)
  return b, m
 ```

**Scikit-Learn**

```python
from sklearn.linear_model import LinearRegression
```

Create a ```LinearRegression``` model, and then fit it to your ```x``` and ```y``` data:

```python
line_fitter = LinearRegression()
line_fitter.fit(X, y)
```

The ```.fit()``` method gives the model two variables that are useful to us:

1. the ```line_fitter.coef_```, which contains the slope

2. the ```line_fitter.intercept_```, which contains the intercept

Use the ```.predict()``` function to pass in x-values and receive the y-values that this line would predict:

```python
y_predicted = line_fitter.predict(X)
```

## Multiple Linear Regression <a name="MLR"/>


